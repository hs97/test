{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bae82a9-d828-4778-866f-bb33846c12db",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "## TYPE YOUR NAME HERE\n",
    "\n",
    "In this course we will be using Python for writing code to apply machine learning and text analysis methods to economics topics. Python is free, flexible, offers a variety of predefined packages, and is popular. It can handle everything from the statistical analysis of Stata to the matrix algebra and simulation of Matlab.\n",
    "\n",
    "This assignment is meant to introduce you to how we will be using Python in this course. For this assignment, you should write/type your answers into this worksheet. You may discuss the problem set with your class mates, but every student must do their own work. \n",
    "\n",
    "It is always important to cite our references that help us in our work. Please list the students you work with here:\n",
    "\n",
    "1\\. \n",
    "\n",
    "2\\.\n",
    "\n",
    "3\\."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1be8a",
   "metadata": {},
   "source": [
    "### I. PRELIMINARIES (20 points total)\n",
    "\n",
    "Preliminaries are listed in the HW1 Repository README.md. It includes the following:\n",
    "\n",
    "* Downloading and installing Python/Anaconda\n",
    "\n",
    "* Installing necessary pacakges for the homework assignment\n",
    "\n",
    "* Setting up your GitHub account and connecting it to GitHub Classroom for Econ 1680\n",
    "\n",
    "* How to sumbit your homework assignment and code (including how to turn your .ipynb file into a .pdf to submit on Canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1accb-310f-47d3-9fac-6c5d7029395a",
   "metadata": {},
   "source": [
    "### II.\tNUMERICAL DATA (40 points total)\n",
    "\n",
    "1\\. Access Zillow Real Estate Data using the Nasdaq Data Link API. Nasdaq Data Link is a dataset aggregation website that also has other economics datasets. These types of websites can make it easier to get data and to explore what types of datasets are available. \n",
    "    \n",
    "a. Set up free account with Nasdaq Data Link (https://data.nasdaq.com/). Find your API Key in your Account Settings. You will need this to download the data.\n",
    "\n",
    "b. Find the “Zillow Real Estate Data” that is Free (https://data.nasdaq.com/databases/ZILLOW/data) This will be the data you will download.\n",
    "\n",
    "c. Click on the “Usage” tab, then select the “Python” sub-tab for instructions on using the Nasdaq Data Link API.\n",
    "\n",
    "d. To decide which variables and regions we want to download data for, we will first download information on the indicators and regions. In a python environment, you will run the code above to import packages, setup your API connection, and download the indicator and region dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033393dc-a223-4d56-a609-d18f0c64a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nasdaqdatalink\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Change the API key to yours\n",
    "nasdaqdatalink.ApiConfig.api_key = 'YOUR_API_KEY'\n",
    "df_zillow_indicators = nasdaqdatalink.get_table('ZILLOW/INDICATORS', paginate=True)\n",
    "df_zillow_regions = nasdaqdatalink.get_table('ZILLOW/REGIONS', paginate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6581f-0f58-4fb4-8153-a0ca7605c633",
   "metadata": {},
   "source": [
    "i. What does ZVHI in the indicator descriptions stand for? (1 point)\n",
    "\n",
    "ii. What is the indicator, description, and category of row 38 in df_zillow_indicators? Hint: use `.iloc[]` (2 points)\n",
    "\n",
    "iii. In df_zillow_regions, how many regions are there when you search for “Providence; RI”? What is the region_id number for Providence, RI? Hint: use `.str.contains(‘Providence; RI’)` (4 points)\n",
    "\n",
    "iv. Download a dataframe the city of Providence, RI on ZHVI Single-Family Home values with the correct indicator and region IDs entered using the following line:\n",
    "\n",
    "```python\n",
    "df_zillow_sfh = nasdaqdatalink.get_table('ZILLOW/DATA', indicator_id=' ' , region_id=' ',paginate=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44be6077-40cf-4e86-9a87-c1ed1cd67a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d188a-9a3e-4de7-bc9d-435812d24ed1",
   "metadata": {},
   "source": [
    "Answers:\n",
    "\n",
    "i.\n",
    "\n",
    "ii.\n",
    "\n",
    "iii.\n",
    "\n",
    "iv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebaa801-231f-48cb-b815-b82e3838c64d",
   "metadata": {},
   "source": [
    "2\\. Descriptive statistics \n",
    "\n",
    "a. What is the data frequency in df_zillow_sfh? (1 points)\n",
    "    \n",
    "b. What is the median dollar value of a home in df_zillow_sfh? (4 points)\n",
    "    \n",
    "c.\tWhat is the median dollar value of a home in df_zillow_sfh for the year of 2020? Hint: use  `[df_zillow_sfh['date'].dt.year==2020]` (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba99fd1-8807-4e72-8a2f-cf6f058e4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e22cb-0982-467f-a704-0b0f2ab9ab10",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "a.\n",
    "\n",
    "b.\n",
    "\n",
    "c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c09f0-9ee6-430e-a370-ac6fa2f63b14",
   "metadata": {},
   "source": [
    "3\\. Visualize the Data\n",
    "\n",
    "a. Plot a time series graph for values df_zillow_sfh. Be sure to title your graph and label your axes. (7 points)\n",
    "\n",
    "b. Plot time series graph for yearly median values df_zillow_sfh. Be sure to title your graph and label your axes. Hint: you will can create a new dataframe by creating a ‘year’ column using .dt.year and then use `.groupby(by=[‘year’]).median()` to make a yearly dataframe. (10 points)\n",
    "\n",
    "c. What looks different in these graphs? Why? (3 points)\n",
    "    \n",
    "d. Describe the patterns in the graph. What does it say about the housing market in Providence, RI over time? In recent years? What additional data would you need to make claims about what is changing this price? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723748e0-3ee4-4767-8ca0-e9cd9ce36b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475c24d-3c6d-4618-a5fb-b4a401012f17",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "a.\n",
    "\n",
    "b.\n",
    "\n",
    "c.\n",
    "\n",
    "d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf832c-39ba-4143-8790-2e89c72ee11d",
   "metadata": {},
   "source": [
    "### II.\tTEXT DATA (40 points total)\n",
    "    \n",
    "4\\. Download US Economic News Dataset from Kaggle.com: Sign up for a free account with Kaggle.com. This website hosts data science competitions and often has cool datasets available for download. We will be using the US Economic News Dataset at https://www.kaggle.com/heeraldedhia/us-economic-news-articles. Download the CSV file from the website by clicking “Download.”\n",
    "     \n",
    "5\\. Load a subset of the data into Jupyter/Spyder/Python: Sometimes you may be working with a large dataset and it is therefore important to understand how to load a subset of the data at a time. The US Economic News dataset has 8,000 observations.\n",
    "\n",
    "a. Run the code below and explain in words each of the lines of code with comments (use # to comment): (5 points)\n",
    "      \n",
    "b. What code would you write to keep only the ‘date’, ‘headline’, and ‘text’ columns in the dataframe? Run that code. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6b60c-27bb-4247-95cd-9127a8779da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "#\n",
    "folder_path = 'TYPE FILE PATH TO WHERE YOU HAVE THE DATA'\n",
    "\n",
    "#\n",
    "fileReader = open(os.path.join(folder_path, \"US-Economic-News.csv\"), \"r\", encoding=\"unicode_escape\")\n",
    "csvReader = csv.reader(fileReader)\n",
    "\n",
    "#\n",
    "fileWriter = open(os.path.join(folder_path, \"Subset_US_Economic_News.csv\"), \"w\", encoding=\"unicode_escape\", newline='')\n",
    "csvWriter = csv.writer(fileWriter)\n",
    "\n",
    "#\n",
    "acHeader = next(csvReader)\n",
    "csvWriter.writerow(acHeader)\n",
    "\n",
    "#\n",
    "for index, acRow in enumerate(csvReader):\n",
    "    if index < 800:\n",
    "        csvWriter.writerow(acRow)\n",
    "\n",
    "#\n",
    "fileReader.close()\n",
    "fileWriter.close()\n",
    "\n",
    "#\n",
    "df_news = pd.read_csv(os.path.join(folder_path,\"Subset_US_Economic_News.csv\"), encoding='unicode_escape')\n",
    "df_news['date'] = pd.to_datetime(df_news['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa68b4-5c8d-45a8-ad9a-79be7f725d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) for b.:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e9110-f565-418d-b0d1-14713f54987b",
   "metadata": {},
   "source": [
    "6\\. This dataframe is full of text data about US Economic News. When we try to extract information from text, formatting of words and string in code is very important.\n",
    "\n",
    "a. Count the number of headlines that have ‘US’ in them. Hint: loop over `df_news[‘headlines’]`. (3 points)\n",
    "    \n",
    "b. Count the number of headlines that have ‘us’ in them. (3 points)\n",
    "    \n",
    "c. Why are these counts different? Hint: tell python to check if ‘us’ is in the string ‘trust’. Then tell python to check if ‘ us ‘ is in the string ‘trust’. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57089a48-fb8d-4b1a-8d1b-e8fe02d0bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b284a39-76fb-4ef3-a52d-737e83e8db0a",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "a.\n",
    "\n",
    "b.\n",
    "\n",
    "c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412a30c-3bd1-473d-8c73-ca99abe8f634",
   "metadata": {},
   "source": [
    "7\\. In text analysis, we will need to perform a few tasks to clean the data to prepare it for consistent analysis. Run the code and explain what each line does as comments (use # to comment): (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88807593-27ec-433e-a98a-af5b3db35126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "#\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~') \n",
    "\n",
    "#\n",
    "token_list = []\n",
    "for i, row in enumerate(df_news['text']):\n",
    "    text = row.translate(table_punctuation)\n",
    "    tokens = [word.lower() for word in nltk.tokenize.word_tokenize(text) if word.lower() not in stops]\n",
    "    token_list.append(tokens)\n",
    "\n",
    "#\n",
    "df_news['tokens'] = token_list\n",
    "\n",
    "#\n",
    "monetary_policy_wordlist = ['monetary', 'fed ', 'federal reserve', 'Federal Reserve', 'Monetary']\n",
    "\n",
    "#\n",
    "tally = 0\n",
    "monetary_text = []\n",
    "for row in df_news['text']:\n",
    "    mon = 0\n",
    "    if any(keyword in row for keyword in monetary_policy_wordlist):\n",
    "        tally += 1\n",
    "        mon = 1\n",
    "    monetary_text.append(mon)\n",
    "print(tally)\n",
    "df_news['monetary_flag'] = monetary_text\n",
    "\n",
    "#\n",
    "df_monetarynews = df_news[df_news['monetary_flag']==1]\n",
    "df_nonmonetarynews = df_news[df_news['monetary_flag']!=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fda93-8a92-4dc7-8d4d-a54d8ab7bd38",
   "metadata": {},
   "source": [
    "8\\. Compare and contrast the news articles about monetary policy in the US and those about non-monetary-policy economics in the US.\n",
    "\n",
    "a. This code calculates the top 30 most common words in df_news. \n",
    "```python\n",
    "from collections import Counter\n",
    "top_N = 30\n",
    "words = [word for tokenlist in df_news['tokens'].tolist() for word in tokenlist]\n",
    "topwords = pd.DataFrame(Counter(words).most_common(top_N),\n",
    "                           columns=['Word', 'Count']).set_index('Word')\n",
    "print(topwords)\n",
    "```  \n",
    "Adapt it for the following subquestions:\n",
    "\n",
    "i. What are the 15 most common words from df_monetarynews? (3 points)\n",
    "\n",
    "ii. What are the 15 most common words from df_nonmonetarynews? (3 points)      \n",
    "\n",
    "iii. What differences do you notice? (1 points) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f74d0d-830a-4d28-8707-86d613a39e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b554133-18ed-4d69-ae7d-f9afde31d199",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "i.\n",
    "\n",
    "ii.\n",
    "\n",
    "iii."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ce7de-c88e-4c0b-903a-187d2ff6aad7",
   "metadata": {},
   "source": [
    "b. Building on the code from part a, you will visualize the word use in the different types of articles using a word cloud. Below is the code for making the word cloud for the df_news dataframe. You must adapt it to the other dataframes: \n",
    "```python\n",
    "from wordcloud import WordCloud\n",
    "allwords = ' '.join(words)\n",
    "word_cloud = WordCloud(collocations=False, background_color='white').generate(allwords)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Word Cloud for US Economics Articles')\n",
    "plt.show()\n",
    "``` \n",
    "    \n",
    "i. What is the word cloud for df_monetarynews? (3 points)\n",
    "\n",
    "ii. What is the word cloud df_nonmonetarynews? (3 points)      \n",
    "\n",
    "iii. What differences do you notice? Do these differences seem consistent with your list of top 15 most common words? (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e593c8-45d8-4918-a0dd-9662cbf21bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3283-2ee5-4ef8-98f7-e6f637e00a56",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "i.\n",
    "\n",
    "ii.\n",
    "\n",
    "iii."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5b7a9-bc8f-4c37-82cb-5523bb0c9ad6",
   "metadata": {},
   "source": [
    "9\\. Monetary Uncertainty in the News: Loughran and McDonald (2011) have created a commonly used bank of word-sentiment lists. One list is a list of “uncertainty words” You can find this dataset in the Github HW1 Repository. The following is code to make a Monetary Uncertain Score from df_monetarynews and to plot the figure over time. However, there are three things wrong with in the code. Identify the typos, run the correct code, and insert the graph below. HINT: Run the code line by line and manually view the objects that were created and/or the error codes that appear. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6900bbc-6f3a-4b10-baf2-c68578211563",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'TYPE FILE PATH TO WHERE YOU HAVE THE DATA'\n",
    "\n",
    "# Word Lists\n",
    "uncertainty_wordlist_LM = pd.read_csv(os.path.join(folder_path,\"LM_Uncertainty.csv\"), encoding='utf-8')\n",
    "uncertainty_wordlist_LM = uncertainty_wordlist_LM['uncertain words'].tolist()\n",
    "\n",
    "# Text Uncertainty Score for Each Article\n",
    "uncertainty_score = []\n",
    "for row in df_monetarynews['tokens']:\n",
    "    u_tally = 0\n",
    "    for word in uncertainty_wordlist_LM:\n",
    "        if word in row:\n",
    "            u_tally += 1\n",
    "uncertainty_score.append(u_tally)\n",
    "                \n",
    "df_monetarynews['text_uncertainty_score'] = uncertainty_score     \n",
    "    \n",
    "# Plot Yearly Mean Monetary Policy Uncertainty Over Time\n",
    "\n",
    "#Take mean over years\n",
    "df_monetarynews['year'] = df_monetarynews['date'].dt.year.astype(str)\n",
    "df_monetarynews_yearly = df_monetarynews.groupby(by=['year']).average()\n",
    "\n",
    "df_monetarynews_yearly['year'] = pd.to_datetime(df_monetarynews_yearly.index)\n",
    "\n",
    "#Plot time Series\n",
    "plt.plot(df_monetarynews_yearly['year'],df_monetarynews['text_uncertainty_score'])\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Mean Uncertainty Score, Yearly')\n",
    "plt.title('Uncertainty in US Monetary Policy News Articles')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
